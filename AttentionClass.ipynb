{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saranshikens/Basic-ML/blob/main/AttentionClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b92c1cb",
      "metadata": {
        "id": "7b92c1cb"
      },
      "source": [
        "$\\Huge \\text{ATTENTION LAYERS}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98b7cc5e",
      "metadata": {
        "id": "98b7cc5e"
      },
      "source": [
        "●\tCNNs  \n",
        "●\tRNNs  \n",
        "●\tLSTM  \n",
        "●\tSequence to Sequence Modelling  \n",
        "●\tIntro to Attention\n",
        "\n",
        "In this notebook, I will define 4 different kinds of attention mechanisms.  \n",
        "These will be used in conjuction with 4 architectures, namely RNNs and LSTMs (both uni-directional and bi-directional), in Models.ipynb    \n",
        "The classes have been created in such a way that they can be readily imported into the architectures and  \n",
        "are compatible with pytorch.  \n",
        "Each class accepts encoder hidden states and decoder state and returns context vector and attention weights.  \n",
        "By - Saransh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f5827c1",
      "metadata": {
        "id": "7f5827c1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XPV0wn4ljOK-"
      },
      "id": "XPV0wn4ljOK-"
    },
    {
      "cell_type": "markdown",
      "id": "e3cada20",
      "metadata": {
        "id": "e3cada20"
      },
      "source": [
        "$\\Large \\text{Theory}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbaec317",
      "metadata": {
        "id": "bbaec317"
      },
      "source": [
        "For all the attention architectures mentioned below:  \n",
        "Let the annotation generated by the encoder, for every word $x_i$ in an input sentence of length $T$, be $h_i$ (key),   \n",
        "and the previous hidden decoder state be $s_{t-1}$ (query).  \n",
        "The decoder takes each $h_i$ and feeds it to an alignment model, $a(.)$, together with $s_{t-1}$.  \n",
        "This generates an attention score: $e_{t,i} = a(s_{t-1}, h_i)$.  \n",
        "For each architecture, it is the alignment model that will change.  \n",
        "A softmax function is appplied to each attention score to obtain the corresponding weight: $\\alpha_{t,i} = \\text{softmax}(e_{t,i})$.  \n",
        "The context vector is given as a weighted sum of the annotations: $\\displaystyle c_t = \\sum_{i=1}^{T} \\alpha_{t,i} h_i$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "W9Bux5r5jPgd"
      },
      "id": "W9Bux5r5jPgd"
    },
    {
      "cell_type": "markdown",
      "id": "1811d7e1",
      "metadata": {
        "id": "1811d7e1"
      },
      "source": [
        "$\\Large \\text{Bahdanau Attention}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d61ca20a",
      "metadata": {
        "id": "d61ca20a"
      },
      "source": [
        "The alignment model combines $s_{t-1}$ and $h_i$ by either a weighted addition, or concatenation.  \n",
        "Here, $a(s_{t-1},h_i) = \\textbf{v}^T \\text{tanh}(\\textbf{W}[h_i;s_{t-1}])$, or  \n",
        "$a(s_{t-1},h_i) = \\textbf{v}^T \\text{tanh}(\\textbf{W}_1 h_i + \\textbf{W}_2 s_{t-1})$, where $\\textbf{v}$ is a weight vector and $\\textbf{W}, \\textbf{W}_1, \\textbf{W}_2$ are weight matrices.  \n",
        "These parameters will be learned by the model.  \n",
        "I have implemented the second version of Bahdanau Attention here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ec791d9",
      "metadata": {
        "id": "5ec791d9"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, attn_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = nn.Linear(hidden_size, attn_size)\n",
        "        self.W2 = nn.Linear(hidden_size, attn_size)\n",
        "        self.v = nn.Linear(attn_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, key, query):\n",
        "        # dimension of encoder_ouput is [batch_size, seq_len, hidden_size]\n",
        "        # dimension of decoder_output is [batch_size, hidden_len]\n",
        "        # to add these two, their dimension must be equal, so we 'unsqueeze' the\n",
        "        # decoder_output, so that its 2nd dimension is sequence_len\n",
        "        sequence_len = key.size(1) # length of input sentence\n",
        "        query = query.unsqueeze(1).repeat(1, sequence_len, 1)\n",
        "\n",
        "        score = torch.tanh(self.W1(key) + self.W2(query))\n",
        "        # dimension of context vector should be  (batch_size, hidden_size), so\n",
        "        # we squeeze and unsqueeze accordingly\n",
        "        attention_weights = F.softmax(self.v(score).squeeze(-1), dim=1)\n",
        "\n",
        "        context_vector = torch.bmm(attention_weights.unsqueeze(1), key).squeeze(1)\n",
        "\n",
        "        return attention_weights, context_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "KlvzCV4LjQr9"
      },
      "id": "KlvzCV4LjQr9"
    },
    {
      "cell_type": "markdown",
      "id": "a72192d4",
      "metadata": {
        "id": "a72192d4"
      },
      "source": [
        "$\\Large \\text{Luong Dot Attention}$  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53e8a01f",
      "metadata": {
        "id": "53e8a01f"
      },
      "source": [
        "Here, $a(s_t, h_i) = s_{t}^{T} h_i$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e19ad446",
      "metadata": {
        "id": "e19ad446"
      },
      "outputs": [],
      "source": [
        "class LuongDotAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LuongDotAttention, self).__init__()\n",
        "\n",
        "    def forward(self, key, query):\n",
        "        query = query.unsqueeze(2)\n",
        "        att_scores = torch.bmm(key, query).squeeze(2)\n",
        "        att_weights = F.softmax(att_scores, dim=1)\n",
        "        context_vector = torch.bmm(att_weights.unsqueeze(1), key).squeeze(1)\n",
        "\n",
        "        return att_weights, context_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "A7DmRrFkjRsg"
      },
      "id": "A7DmRrFkjRsg"
    },
    {
      "cell_type": "markdown",
      "id": "65bf008f",
      "metadata": {
        "id": "65bf008f"
      },
      "source": [
        "$\\Large \\text{Luong General Attention}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d3a6aff",
      "metadata": {
        "id": "7d3a6aff"
      },
      "source": [
        "Here, $a(s_t, h_i) = s_{t}^{T}\\textbf{W}_a h_i$, where $\\textbf{W}_a$ is learned by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8d4606b",
      "metadata": {
        "id": "b8d4606b"
      },
      "outputs": [],
      "source": [
        "class LuongGeneralAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(LuongGeneralAttention, self).__init__()\n",
        "        self.W = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, key, query):\n",
        "        transformed_key = self.W(key)\n",
        "        query = query.unsqueeze(2)\n",
        "        att_scores = torch.bmm(transformed_key, query).squeeze(2)\n",
        "        att_weights = F.softmax(att_scores, dim=1)\n",
        "        context_vector = torch.bmm(att_weights.unsqueeze(1), key).squeeze(1)\n",
        "\n",
        "        return att_weights, context_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rD-UjvlkjUP7"
      },
      "id": "rD-UjvlkjUP7"
    },
    {
      "cell_type": "markdown",
      "id": "bb46e291",
      "metadata": {
        "id": "bb46e291"
      },
      "source": [
        "$\\Large \\text{Luong Concat Attention}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0dbca76",
      "metadata": {
        "id": "e0dbca76"
      },
      "source": [
        "Here, $a(s_t,h_i) = \\textbf{v}^T \\text{tanh}(\\textbf{W}[h_i;s_t])$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29758b5c",
      "metadata": {
        "id": "29758b5c"
      },
      "outputs": [],
      "source": [
        "class LuongConcatAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, attn_size):\n",
        "        super(LuongConcatAttention, self).__init__()\n",
        "        # Input to the attention is the concatenation → size is 2 * hidden_size\n",
        "        self.W = nn.Linear(2 * hidden_size, attn_size)\n",
        "        self.v = nn.Linear(attn_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, key, query):\n",
        "        batch_size, seq_len, hidden_size = key.size()\n",
        "        query = query.unsqueeze(1).repeat(1, seq_len, 1)\n",
        "        concat_matrix = torch.cat((key, query), dim=2)\n",
        "        scores = torch.tanh(self.W(concat_matrix))\n",
        "        att_scores = self.v(scores).squeeze(2)\n",
        "        att_weights = F.softmax(att_scores, dim=1)\n",
        "        context_vector = torch.bmm(att_weights.unsqueeze(1), key).squeeze(1)\n",
        "\n",
        "        return att_weights, context_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To look at the models, run Models.ipynb"
      ],
      "metadata": {
        "id": "vO0FKPHrjVrE"
      },
      "id": "vO0FKPHrjVrE"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}