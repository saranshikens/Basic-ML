{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1913,
          "sourceType": "datasetVersion",
          "datasetId": 1057
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saranshikens/Basic-ML/blob/main/Recurrent_Neural_Network_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPLEMENTING RECURRENT NEURAL NETWORKS FROM SCRATCH**"
      ],
      "metadata": {
        "id": "3wmqwwLBnuqj"
      }
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "path = kagglehub.dataset_download('rakannimer/air-passengers')\n",
        "\n",
        "print('Data source import complete.')\n",
        "\n",
        "csv_file_path = None\n",
        "for root, dirs, files in os.walk(path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".csv\"):\n",
        "            csv_file_path = os.path.join(root, file)\n",
        "            break\n",
        "    if csv_file_path:\n",
        "        break\n",
        "\n",
        "if csv_file_path:\n",
        "    data = pd.read_csv(csv_file_path)\n",
        "    print(\"CSV file loaded successfully.\")\n",
        "else:\n",
        "    print(\"No CSV file found in the downloaded directory.\")\n"
      ],
      "metadata": {
        "id": "vdJzS_58cNjZ",
        "outputId": "da9a562d-5cd6-482c-9cef-ac5907a9cf57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n",
            "CSV file loaded successfully.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN:\n",
        "  def __init__(self, input_size, hidden_size, output_size, lr, n_iter):\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.lr = lr\n",
        "    self.n_iter = n_iter\n",
        "    self.initialize()\n",
        "\n",
        "  def initialize(self):\n",
        "    self.W_input_hidden = np.random.randn(self.hidden_size, self.input_size)*0.01\n",
        "    self.W_hidden_hidden = np.random.randn(self.hidden_size, self.hidden_size)*0.01\n",
        "    self.W_hidden_output = np.random.randn(self.output_size, self.hidden_size)*0.01\n",
        "    self.b_hidden = np.zeros((self.hidden_size, 1))\n",
        "    self.b_output = np.zeros((self.output_size, 1))\n",
        "\n",
        "  def tanh(self, X):\n",
        "    return np.tanh(X)\n",
        "\n",
        "  def tanh_deriv(self, X):\n",
        "    return 1 - np.tanh(X)**2\n",
        "\n",
        "  def forward_prop(self, inputs):\n",
        "    hidden_layer = np.zeros((self.hidden_size, 1))\n",
        "    hidden_layers = {0: hidden_layer}\n",
        "    for time in range(len(inputs)):\n",
        "      X = inputs[time].reshape(-1, 1)\n",
        "      hidden_layer = self.tanh(np.dot(self.W_input_hidden, X) + np.dot(self.W_hidden_hidden, hidden_layer) + self.b_hidden)\n",
        "      hidden_layers[time+1] = hidden_layer\n",
        "    y_pred = np.dot(self.W_hidden_output, hidden_layer) + self.b_output\n",
        "    return y_pred, hidden_layers\n",
        "\n",
        "  def backward_prop(self, inputs, target, y_pred, hidden_layers):\n",
        "    d_W_hidden_output = np.zeros_like(self.W_hidden_output)\n",
        "    d_W_hidden_hidden = np.zeros_like(self.W_hidden_hidden)\n",
        "    d_W_input_hidden = np.zeros_like(self.W_input_hidden)\n",
        "    d_b_hidden = np.zeros_like(self.b_hidden)\n",
        "    d_b_output = np.zeros_like(self.b_output)\n",
        "    d_hidden_layer_next = np.zeros_like(hidden_layers[0])\n",
        "\n",
        "    d_y = y_pred - target.reshape(-1,1)\n",
        "    d_W_hidden_output += np.dot(d_y, hidden_layers[len(inputs)].T)\n",
        "    d_b_output += d_y\n",
        "\n",
        "    for time in reversed(range(len(inputs))):\n",
        "      d_hidden = np.dot(self.W_hidden_output.T, d_y) + d_hidden_layer_next\n",
        "      d_tanh = d_hidden * self.tanh_deriv(hidden_layers[time+1])\n",
        "      d_b_hidden += d_tanh\n",
        "      d_W_input_hidden += np.dot(d_tanh, inputs[time].reshape(1,-1))\n",
        "      d_W_hidden_hidden += np.dot(d_tanh, hidden_layers[time].T)\n",
        "      d_hidden_layer_next = np.dot(self.W_hidden_hidden.T, d_tanh)\n",
        "\n",
        "    return d_W_input_hidden, d_W_hidden_hidden, d_W_hidden_output, d_b_hidden, d_b_output\n",
        "\n",
        "  def update(self, d_W_input_hidden, d_W_hidden_hidden, d_W_hidden_output, d_b_hidden, d_b_output):\n",
        "    self.W_input_hidden -= self.lr * d_W_input_hidden\n",
        "    self.W_hidden_hidden -= self.lr * d_W_hidden_hidden\n",
        "    self.W_hidden_output -= self.lr * d_W_hidden_output\n",
        "    self.b_hidden -= self.lr * d_b_hidden\n",
        "    self.b_output -= self.lr * d_b_output\n",
        "\n",
        "  def train(self, X, y):\n",
        "    for i in range(self.n_iter):\n",
        "      loss = 0\n",
        "      for x_seq, y_true in zip(X, y):\n",
        "        y_pred, hidden_layers = self.forward_prop(x_seq)\n",
        "        loss += np.mean((y_pred - y_true.reshape(-1,1))**2)\n",
        "        gradients = self.backward_prop(x_seq, y_true, y_pred, hidden_layers)\n",
        "        self.update(*gradients)\n",
        "      if i%10 == 0:\n",
        "        print(f\"Iteration {i}: MSE Loss = {loss/len(X):.4f}\")\n",
        "\n",
        "  def predict(self, X_seq):\n",
        "    y_pred, _ = self.forward_prop(X_seq)\n",
        "    return y_pred.flatten()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-06T18:29:25.175397Z",
          "iopub.execute_input": "2025-06-06T18:29:25.175686Z",
          "iopub.status.idle": "2025-06-06T18:29:25.19099Z",
          "shell.execute_reply.started": "2025-06-06T18:29:25.175666Z",
          "shell.execute_reply": "2025-06-06T18:29:25.190058Z"
        },
        "id": "Lh1mQsErcNjh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREPROCESSING THE DATA**"
      ],
      "metadata": {
        "id": "Qvpfxbnxn5Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Month'] = pd.to_datetime(data['Month'])\n",
        "data.set_index('Month', inplace=True)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "data['#Passengers'] = scaler.fit_transform(data[['#Passengers']])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-06T18:28:50.774075Z",
          "iopub.execute_input": "2025-06-06T18:28:50.774664Z",
          "iopub.status.idle": "2025-06-06T18:28:50.778654Z",
          "shell.execute_reply.started": "2025-06-06T18:28:50.774637Z",
          "shell.execute_reply": "2025-06-06T18:28:50.777682Z"
        },
        "id": "Ec4WwYCPcNjl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CREATING SEQUENCES**"
      ],
      "metadata": {
        "id": "UAvG77Sln8zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, seq_length):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 12\n",
        "data = data['#Passengers'].values\n",
        "X, y = create_sequences(data, seq_length)\n",
        "\n",
        "split = int(0.8 * len(X))\n",
        "X_train, y_train = X[:split], y[:split]\n",
        "X_test, y_test = X[split:], y[split:]\n",
        "\n",
        "X_train_seq = [x.reshape(-1, 1) for x in X_train]\n",
        "X_test_seq = [x.reshape(-1, 1) for x in X_test]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-06T18:27:05.879782Z",
          "iopub.execute_input": "2025-06-06T18:27:05.880153Z",
          "iopub.status.idle": "2025-06-06T18:27:05.888546Z",
          "shell.execute_reply.started": "2025-06-06T18:27:05.880121Z",
          "shell.execute_reply": "2025-06-06T18:27:05.887673Z"
        },
        "id": "4VLXScGfcNjm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAINING THE MODEL**"
      ],
      "metadata": {
        "id": "X9YccJpfn_YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = RNN(input_size=1, hidden_size=16, output_size=1, lr=0.01, n_iter=100)\n",
        "rnn.train(X_train_seq, y_train)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-06T18:29:28.578256Z",
          "iopub.execute_input": "2025-06-06T18:29:28.578555Z",
          "iopub.status.idle": "2025-06-06T18:29:28.597195Z",
          "shell.execute_reply.started": "2025-06-06T18:29:28.578533Z",
          "shell.execute_reply": "2025-06-06T18:29:28.596029Z"
        },
        "id": "aLdFxjXVcNjn",
        "outputId": "f3fe83d7-8055-409d-ce34-fcd51c1dc2b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: MSE Loss = 0.0588\n",
            "Iteration 10: MSE Loss = 0.0287\n",
            "Iteration 20: MSE Loss = 0.0201\n",
            "Iteration 30: MSE Loss = 0.0058\n",
            "Iteration 40: MSE Loss = 0.0061\n",
            "Iteration 50: MSE Loss = 0.0058\n",
            "Iteration 60: MSE Loss = 0.0057\n",
            "Iteration 70: MSE Loss = 0.0081\n",
            "Iteration 80: MSE Loss = 0.0090\n",
            "Iteration 90: MSE Loss = 0.0091\n"
          ]
        }
      ],
      "execution_count": null
    }
  ]
}